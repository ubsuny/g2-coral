{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interpreted-award",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import libraries we need\n",
    "%pylab inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../py_programs/')\n",
    "from Simulator import *\n",
    "from keract import get_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-activation",
   "metadata": {},
   "source": [
    "### load data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hollow-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from files\n",
    "filenames = []\n",
    "x_ = []\n",
    "y_ = []\n",
    "\n",
    "# iterate over all files\n",
    "#filenames = ['./../simulation/data/laser'] + ['./../simulation/data/sps']\n",
    "filenames = ['./../simulation/data/halfhalf_rand/'+str(i) for i in range(50)]\n",
    "\n",
    "\n",
    "for filename in filenames: \n",
    "    data = load_data(filename=filename)\n",
    "    x_.append(data)\n",
    "    \n",
    "x_ = np.array(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blond-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into correct shape\n",
    "x_ = x_.reshape(x_.shape[0]*x_.shape[1],x_.shape[2],x_.shape[3])\n",
    "\n",
    "# get the ground truth\n",
    "threshold = 0.5\n",
    "y_ = get_truth(x_, thr=threshold)\n",
    "\n",
    "# we only need the g2signal for training\n",
    "x_ = x_[:,2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-council",
   "metadata": {},
   "source": [
    "Now we've collected all the datasets. For training the model, we need to **shuffle** all data sets first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fixed-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "i = np.random.permutation(len(x_))\n",
    "x_ = x_[i]\n",
    "y_ = y_[i]\n",
    "\n",
    "# reshape\n",
    "x_ = x_.reshape(x_.shape[0], x_.shape[1], 1)\n",
    "y_ = y_.reshape(y_.shape[0])\n",
    "\n",
    "# allocate\n",
    "x_train = x_[100:]\n",
    "y_train = y_[100:]\n",
    "x_test = x_[:100]\n",
    "y_test = y_[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-parliament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5859183673469388\n",
      "0.54\n"
     ]
    }
   ],
   "source": [
    "# check the sps rate (data sets considered as sps out of all data sets) \n",
    "print(y_train[y_train==1].shape[0]/y_train.shape[0])\n",
    "print(y_test[y_test==1].shape[0]/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-identity",
   "metadata": {},
   "source": [
    "### get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "meaning-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from files\n",
    "filenames = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# iterate over all files\n",
    "filenames = ['./../simulation/data/test_data/'+str(i) for i in range(20)]\n",
    "#filenames = ['./../simulation/data/halfhalf_rand/'+str(i) for i in range(50)]\n",
    "\n",
    "#filenames = ['./../simulation/data/sps'] + ['./../simulation/data/laser']\n",
    "\n",
    "\n",
    "for filename in filenames: \n",
    "    data = load_data(filename=filename)\n",
    "    x_test.append(data)\n",
    "    \n",
    "x_test = np.array(x_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "medium-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert simulated data into correct shape\n",
    "x_test = x_test.reshape(x_test.shape[0]*x_test.shape[1],x_test.shape[2],x_test.shape[3])\n",
    "\n",
    "# get the ground truth\n",
    "threshold = 0.5\n",
    "y_test = get_truth(x_test, thr=threshold)\n",
    "\n",
    "# we only need the g2signal\n",
    "x_test = x_test[:,1,:]\n",
    "\n",
    "\n",
    "# shuffle\n",
    "i = np.random.permutation(len(x_test))\n",
    "x_test = x_test[i]\n",
    "y_test = y_test[i]\n",
    "\n",
    "# reshape\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "y_test = y_test.reshape(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-verse",
   "metadata": {},
   "source": [
    "---\n",
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    # input shape should be (time signal, 1)\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # 1st con1d layer\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=4, padding='valid', strides=1, activation='relu')(input_layer)\n",
    "    \n",
    "    # 2nd con1d layer\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=4, padding='valid', strides=1, activation='relu')(conv1)\n",
    "\n",
    "    # 3rd con1d layer\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=4, padding='valid', strides=1, activation='relu')(conv2)\n",
    "    \n",
    "    # maxpooling layer\n",
    "    pool = keras.layers.MaxPool1D(pool_size=3, strides=2, padding='valid')(conv3) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    # flatten layer\n",
    "    flat = keras.layers.Flatten()(pool)\n",
    "    \n",
    "    # fully connected layer to output a binary vector\n",
    "    dense1 = keras.layers.Dense(2, activation='relu')(flat)\n",
    "    #dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=dense1)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=x_test.shape[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-sport",
   "metadata": {},
   "source": [
    "# currently working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "driving-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current working on\n",
    "def make_model2(input_shape):\n",
    "    # input shape should be (vinvalues, 1)\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "     \n",
    "    lam1 = keras.layers.Lambda(lambda x: -x)(input_layer)\n",
    "    \n",
    "    min1 = keras.layers.MaxPool1D(pool_size=4,strides=5, padding='same')(lam1)\n",
    "    \n",
    "    lam2 = keras.layers.Lambda(lambda x: -x)(min1)\n",
    "    \n",
    "    conv1 = keras.layers.Conv1D(filters=4, kernel_size=3, padding='same', strides=1)(lam2)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    # maxpooling layer\n",
    "    #pool = keras.layers.MaxPool1D(pool_size=5, strides=5, padding='same')(conv1) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    # flatten layer\n",
    "    flat = keras.layers.Flatten()(conv1)\n",
    "    \n",
    "    #pool = keras.layers.GlobalAveragePooling1D()(conv1) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    \n",
    "    # fully connected layer to output a binary vector\n",
    "    #dense1 = keras.layers.Dense(10, activation='relu')(flat)\n",
    "    dense2 = keras.layers.Dense(1, activation='sigmoid')(flat)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=dense2)\n",
    "\n",
    "\n",
    "model = make_model2(input_shape=x_.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model3(input_shape):\n",
    "    # input shape should be (time signal, 1)\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "     \n",
    "    #lam1 = keras.layers.Lambda(lambda x: -x)(input_layer)\n",
    "    \n",
    "    #min1 = keras.layers.MaxPool1D(pool_size=4,strides=2,padding='same')(lam1)\n",
    "    \n",
    "    #lam2 = keras.layers.Lambda(lambda x: -x)(min1)\n",
    "    \n",
    "    conv1 = keras.layers.Conv1D(filters=1, kernel_size=5, padding='same', strides=1, activation='relu')(input_layer)\n",
    "    \n",
    "    # maxpooling layer\n",
    "    #pool = keras.layers.MaxPool1D(pool_size=5, strides=5, padding='same')(lam2) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    # flatten layer\n",
    "    #flat = keras.layers.Flatten()(input_layer)\n",
    "    \n",
    "    # fully connected layer to output a binary vector\n",
    "    dense1 = keras.layers.Dense(2, activation='relu')(conv1)\n",
    "    #dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=dense1)\n",
    "\n",
    "\n",
    "model = make_model2(input_shape=x_.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model4(input_shape):\n",
    "    # input shape should be (time signal, 1)\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "     \n",
    "    lam1 = keras.layers.Lambda(lambda x: -x)(input_layer)\n",
    "    \n",
    "    min1 = keras.layers.MaxPool1D(pool_size=4,strides=5, padding='same')(lam1)\n",
    "    \n",
    "    lam2 = keras.layers.Lambda(lambda x: -x)(min1)\n",
    "    \n",
    "    conv1 = keras.layers.Conv1D(filters=4, kernel_size=3, padding='same', strides=1)(lam2)\n",
    "    #conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    # maxpooling layer\n",
    "    #pool = keras.layers.MaxPool1D(pool_size=5, strides=5, padding='same')(conv1) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    # flatten layer\n",
    "    flat = keras.layers.Flatten()(conv1)\n",
    "    \n",
    "    #pool = keras.layers.GlobalAveragePooling1D()(conv1) # keras.layers.GlobalAveragePooling1D()(conv3) #  # \n",
    "    \n",
    "    \n",
    "    # fully connected layer to output a binary vector\n",
    "    #dense1 = keras.layers.Dense(10, activation='relu')(flat)\n",
    "    #dense2 = keras.layers.Dense(1, activation='sigmoid')(flat)\n",
    "    dense2 = keras.layers.Dense(1)(flat)\n",
    "    dense2 = keras.layers.ReLU(max_value=)(dense2)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=dense2)\n",
    "\n",
    "\n",
    "model = make_model2(input_shape=x_.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subsequent-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 200, 1)]          0         \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 200, 1)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 40, 1)             0         \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 40, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 40, 4)             16        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40, 4)             16        \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 40, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 185\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "invalid-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "686/686 [==============================] - 1s 653us/step - loss: 0.1657 - sparse_categorical_accuracy: 0.4269 - binary_accuracy: 0.7581 - val_loss: 0.0622 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9401\n",
      "Epoch 2/30\n",
      "686/686 [==============================] - 0s 525us/step - loss: 0.0493 - sparse_categorical_accuracy: 0.4074 - binary_accuracy: 0.9479 - val_loss: 0.0375 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 3/30\n",
      "686/686 [==============================] - 0s 538us/step - loss: 0.0301 - sparse_categorical_accuracy: 0.4258 - binary_accuracy: 0.9797 - val_loss: 0.0265 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 4/30\n",
      "686/686 [==============================] - 0s 515us/step - loss: 0.0238 - sparse_categorical_accuracy: 0.4292 - binary_accuracy: 0.9801 - val_loss: 0.0221 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 5/30\n",
      "686/686 [==============================] - 0s 512us/step - loss: 0.0183 - sparse_categorical_accuracy: 0.4245 - binary_accuracy: 0.9843 - val_loss: 0.0186 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 6/30\n",
      "686/686 [==============================] - 0s 523us/step - loss: 0.0150 - sparse_categorical_accuracy: 0.4278 - binary_accuracy: 0.9871 - val_loss: 0.0163 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 7/30\n",
      "686/686 [==============================] - 0s 517us/step - loss: 0.0122 - sparse_categorical_accuracy: 0.4062 - binary_accuracy: 0.9891 - val_loss: 0.0161 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9905\n",
      "Epoch 8/30\n",
      "686/686 [==============================] - 0s 524us/step - loss: 0.0121 - sparse_categorical_accuracy: 0.4077 - binary_accuracy: 0.9895 - val_loss: 0.0292 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9687\n",
      "Epoch 9/30\n",
      "686/686 [==============================] - 0s 512us/step - loss: 0.0098 - sparse_categorical_accuracy: 0.4109 - binary_accuracy: 0.9918 - val_loss: 0.0114 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9864\n",
      "Epoch 10/30\n",
      "686/686 [==============================] - 0s 523us/step - loss: 0.0104 - sparse_categorical_accuracy: 0.4115 - binary_accuracy: 0.9917 - val_loss: 0.0139 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9898\n",
      "Epoch 11/30\n",
      "686/686 [==============================] - 0s 524us/step - loss: 0.0096 - sparse_categorical_accuracy: 0.4029 - binary_accuracy: 0.9912 - val_loss: 0.0129 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9837\n",
      "Epoch 12/30\n",
      "686/686 [==============================] - 0s 521us/step - loss: 0.0081 - sparse_categorical_accuracy: 0.4098 - binary_accuracy: 0.9943 - val_loss: 0.0100 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9932\n",
      "Epoch 13/30\n",
      "686/686 [==============================] - 0s 518us/step - loss: 0.0080 - sparse_categorical_accuracy: 0.4086 - binary_accuracy: 0.9933 - val_loss: 0.0105 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9925\n",
      "Epoch 14/30\n",
      "686/686 [==============================] - 0s 520us/step - loss: 0.0075 - sparse_categorical_accuracy: 0.4152 - binary_accuracy: 0.9922 - val_loss: 0.0105 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9959\n",
      "Epoch 15/30\n",
      "686/686 [==============================] - 0s 517us/step - loss: 0.0086 - sparse_categorical_accuracy: 0.4219 - binary_accuracy: 0.9889 - val_loss: 0.0153 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9816\n",
      "Epoch 16/30\n",
      "686/686 [==============================] - 0s 527us/step - loss: 0.0078 - sparse_categorical_accuracy: 0.4167 - binary_accuracy: 0.9925 - val_loss: 0.0153 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9844\n",
      "Epoch 17/30\n",
      "686/686 [==============================] - 0s 542us/step - loss: 0.0066 - sparse_categorical_accuracy: 0.4203 - binary_accuracy: 0.9947 - val_loss: 0.0093 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9898\n",
      "Epoch 18/30\n",
      "686/686 [==============================] - 0s 530us/step - loss: 0.0074 - sparse_categorical_accuracy: 0.4115 - binary_accuracy: 0.9934 - val_loss: 0.0087 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9946\n",
      "Epoch 19/30\n",
      "686/686 [==============================] - 0s 507us/step - loss: 0.0073 - sparse_categorical_accuracy: 0.4075 - binary_accuracy: 0.9895 - val_loss: 0.0091 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9925\n",
      "Epoch 20/30\n",
      "686/686 [==============================] - 0s 519us/step - loss: 0.0064 - sparse_categorical_accuracy: 0.4190 - binary_accuracy: 0.9927 - val_loss: 0.0166 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9769\n",
      "Epoch 21/30\n",
      "686/686 [==============================] - 0s 537us/step - loss: 0.0065 - sparse_categorical_accuracy: 0.4256 - binary_accuracy: 0.9937 - val_loss: 0.0162 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9762\n",
      "Epoch 22/30\n",
      "686/686 [==============================] - 0s 521us/step - loss: 0.0059 - sparse_categorical_accuracy: 0.4353 - binary_accuracy: 0.9943 - val_loss: 0.0087 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9891\n",
      "Epoch 23/30\n",
      "686/686 [==============================] - 0s 529us/step - loss: 0.0060 - sparse_categorical_accuracy: 0.4090 - binary_accuracy: 0.9956 - val_loss: 0.0101 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9925\n",
      "Epoch 24/30\n",
      "686/686 [==============================] - 0s 521us/step - loss: 0.0055 - sparse_categorical_accuracy: 0.4245 - binary_accuracy: 0.9954 - val_loss: 0.0137 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9810\n",
      "Epoch 25/30\n",
      "686/686 [==============================] - 0s 523us/step - loss: 0.0070 - sparse_categorical_accuracy: 0.4155 - binary_accuracy: 0.9926 - val_loss: 0.0108 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9871\n",
      "Epoch 26/30\n",
      "686/686 [==============================] - 0s 518us/step - loss: 0.0053 - sparse_categorical_accuracy: 0.4201 - binary_accuracy: 0.9951 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9925\n",
      "Epoch 27/30\n",
      "686/686 [==============================] - 0s 523us/step - loss: 0.0057 - sparse_categorical_accuracy: 0.4218 - binary_accuracy: 0.9942 - val_loss: 0.0079 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9952\n",
      "Epoch 28/30\n",
      "686/686 [==============================] - 0s 532us/step - loss: 0.0053 - sparse_categorical_accuracy: 0.4228 - binary_accuracy: 0.9947 - val_loss: 0.0079 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9884\n",
      "Epoch 29/30\n",
      "686/686 [==============================] - 0s 513us/step - loss: 0.0054 - sparse_categorical_accuracy: 0.4098 - binary_accuracy: 0.9956 - val_loss: 0.0087 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9905\n",
      "Epoch 30/30\n",
      "686/686 [==============================] - 0s 526us/step - loss: 0.0045 - sparse_categorical_accuracy: 0.3978 - binary_accuracy: 0.9967 - val_loss: 0.0077 - val_sparse_categorical_accuracy: 0.4041 - val_binary_accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 5\n",
    "\n",
    "callbacks = [\n",
    "    # save checkpoints\n",
    "    #keras.callbacks.ModelCheckpoint(\n",
    "    #    \"model2.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    #),\n",
    "    \n",
    "    # if there's no improvement for minimizing losses, which makes the training stagnate\n",
    "    # then reduce the learning rate\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    \n",
    "    # stop training if a monitored metric stops improving\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',  #tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='mse',\n",
    "    metrics=['sparse_categorical_accuracy','binary_accuracy'],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.3,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('g2model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-watts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\",metrics=['sparse_categorical_accuracy','binary_accuracy'])\n",
    "activations = get_activations(model, x_train, auto_compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(k, '->', v.shape, '- Numpy array') for (k, v) in activations.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(activations[list(activations.keys())[0]][0])\n",
    "#plt.plot(activations[\"conv1d_5\"][:,0,0])\n",
    "#plt.plot(activations[list(activations.keys())[4]][1,:,0],label=list(activations.keys())[4])\n",
    "\n",
    "#plt.plot(activations[list(activations.keys())[-1]][:,:],label=list(activations.keys())[-1])\n",
    "#plt.plot(activations[\"max_pooling1d_46\"][900,:,0])\n",
    "#activations[list(activations.keys())[6]]\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-buyer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "attached-beverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1.]\n",
      "[1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "r=np.round(model.predict(x_train[:100]))\n",
    "print(np.squeeze(r))\n",
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "attached-needle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 622us/step - loss: 0.0059 - sparse_categorical_accuracy: 0.4151 - binary_accuracy: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005880405660718679, 0.4151020348072052, 0.9938775300979614]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "interested-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "[0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "r=np.round(model.predict(x_test)[:100])\n",
    "print(np.squeeze(r))\n",
    "print(y_test[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "thirty-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 668us/step - loss: 0.3694 - sparse_categorical_accuracy: 0.4510 - binary_accuracy: 0.5490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3693941533565521, 0.45100000500679016, 0.5490000247955322]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "technological-quebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.549"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_test==1].shape[0]/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extraordinary-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5859183673469388"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train==1].shape[0]/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model.predict(x_[:100])\n",
    "np.round(r.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-mounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt('./simulation/data/s1set10.txt', skiprows=9, max_rows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt('./simulation/data/s1set10.txt', skiprows=9, max_rows=1)\n",
    "binnum = int(np.loadtxt('./simulation/data/s1set10.txt', skiprows=4, usecols=1, max_rows=1, delimiter=':'))\n",
    "plt.plot(np.loadtxt('./simulation/data/s1set10.txt', skiprows=13, max_rows=binnum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "binnum = int(np.loadtxt('./simulation/data/s1set10.txt', skiprows=4, usecols=1, max_rows=1, delimiter=':'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Simulator.simulator(gt=0.5, Nbins=200, width=1., Ndet=1e6, sps=1., laser=0., ther=0., non=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.get_data(s1.distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "x_ = []\n",
    "y_ = []\n",
    "filenames = ['./simulation/data/s1set'+str(i)+'.txt' for i in range(500)]+['./simulation/data/s2set'+str(i)+'.txt' for i in range(500)]\n",
    "\n",
    "for filename in filenames: \n",
    "    data = s2.load_data(filename)\n",
    "    x_.append(data[1])\n",
    "    y_.append(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the keras model into TF lite\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-treasury",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-census",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('g2model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "binnum = np.arange(-100,100)\n",
    "g2 = np.arange(200,400)\n",
    "value = np.arange(400,600)\n",
    "st = np.arange(30)\n",
    "data = np.zeros((30,3,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in st:\n",
    "    data[i] = np.array([binnum,g2,value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in st:\n",
    "    try:\n",
    "        data[i] = np.array([signal,binnum,his])\n",
    "    except:\n",
    "        raise Exception('Input distribution is incorrect, please check the light sources fraction.')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test2' + '.csv'\n",
    "            \n",
    "file = open(filename,'w')  # 'a' for appending\n",
    "file.write('data array format: (N seperate measurements  [binnumber  normalized g2 signal  binvalues]  data) ')\n",
    "file.write('\\n')\n",
    "np.savetxt(file, data[:,2,:], header='histogram values (int)') \n",
    "np.savetxt(file, data[:,1,:], header='normalized g2 signal') \n",
    "np.savetxt(file, data[:,0,:], header='bin number array') \n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self, filename):\n",
    "        gt = np.loadtxt(filename, skiprows=1, max_rows=1)\n",
    "        binnum = int(np.loadtxt(filename, skiprows=4, usecols=1, max_rows=1, delimiter=':'))\n",
    "        g2 = np.loadtxt(filename, skiprows=12, max_rows=binnum)\n",
    "        \n",
    "        return [gt, g2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "        parse = np.loadtxt(filename, skiprows=1, max_rows=1000000)\n",
    "        data = np.zeros((parse.shape[0]//3, 3, parse.shape[1]))\n",
    "        data[:,0,:] = parse[:parse.shape[0]//3,:]\n",
    "        data[:,1,:] = parse[parse.shape[0]//3:parse.shape[0]*2//3,:]\n",
    "        data[:,2,:] = parse[parse.shape[0]*2//3:,:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    input: path + filename\n",
    "    output: data sets, same format with generated data sets\n",
    "    '''\n",
    "    # I must say, this is a stupid way for parsing. I will switch to pandas once available\n",
    "    parse = np.loadtxt(filename, skiprows=1, max_rows=1000000)\n",
    "    data = np.zeros((parse.shape[0]//3, 3, parse.shape[1]))\n",
    "    data[:,0,:] = parse[:parse.shape[0]//3,:]\n",
    "    data[:,1,:] = parse[parse.shape[0]//3:parse.shape[0]*2//3,:]\n",
    "    data[:,2,:] = parse[parse.shape[0]*2//3:,:]\n",
    "    \n",
    "    return data\n",
    "data=load_data('./test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_truth(data,thr):\n",
    "    '''\n",
    "    input: data array, threshhold for classifying sps or not sps\n",
    "    output: a binary result array\n",
    "    '''\n",
    "    signal = data[:,1,:]\n",
    "    binnumber = data[:,2,:]\n",
    "    \n",
    "    # create a 1d-array of g2(0) values\n",
    "    g2zero = np.ndarray.flatten(np.array([signal[i][binnumber[i]==0] for i in range(signal.shape[0])]))\n",
    "    binary = np.zeros(g2zero.shape)\n",
    "    \n",
    "    # if it's smaller than the threshold then make it to 1 (it is a sps)\n",
    "    binary[[g2zero[i]<thr for i in range(len(g2zero))]] = 1\n",
    "    \n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "    signal = data[:,1,:]\n",
    "    binnumber = data[:,2,:]\n",
    "    signal[:,binnumber==0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "    g2zero = np.ndarray.flatten(np.array([signal[i][binnumber[i]==0] for i in range(signal.shape[0])]))\n",
    "    thr = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-screen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-popularity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
